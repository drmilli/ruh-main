# Ruh - AI Product Safety Analyzer

## SOURCE OF TRUTH - Complete System Documentation

This document provides comprehensive documentation for the entire Ruh codebase, including all function-level flows, file relationships, and architectural decisions.

---

## Project Overview

**Ruh** is an AI-powered Chrome extension that analyzes product safety by detecting allergens, PFAS compounds, and other harmful substances in consumer products. The system consists of two main components:

1. **Backend**: Python FastAPI server using Claude AI Agent SDK for product analysis
2. **Extension**: Svelte 5 Chrome extension (Manifest V3) for user interface

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER (Chrome Browser)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXTENSION (/extension)                                          â”‚
â”‚  â”œâ”€ Content Script (content.ts) - Injected into Amazon pages    â”‚
â”‚  â”œâ”€ Background Worker (background.ts) - Service worker          â”‚
â”‚  â””â”€ Sidebar App (Sidebar.svelte) - Analysis UI in iframe        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â†“ HTTP POST /api/analyze
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BACKEND (/backend)                                              â”‚
â”‚  â”œâ”€ API Layer (FastAPI) - HTTP endpoints                        â”‚
â”‚  â”œâ”€ Domain Layer - Business logic & harm scoring                â”‚
â”‚  â””â”€ Infrastructure Layer - Claude AI, DB, scrapers              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Claude AI   â”‚          â”‚  Supabase    â”‚
            â”‚  (Anthropic) â”‚          â”‚  (PostgreSQL)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Complete Function-Level Flow Diagram

### MAIN FEATURE: Product Analysis (User Click â†’ Display Results)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 1: USER VISITS AMAZON PRODUCT PAGE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[User navigates to Amazon product page]
  â†“
ğŸ“„ extension/src/content/content.ts::init()
  â”œâ”€ Calls: isAmazonProductPage(window.location.href) â†’ boolean
  â”œâ”€ Stores: currentProductUrl = window.location.href
  â”œâ”€ Sets up: chrome.runtime.onMessage listener
  â””â”€ Calls: startAnalysis()

ğŸ“„ extension/src/content/content.ts::startAnalysis()
  â”œâ”€ Reads: import.meta.env.VITE_API_BASE_URL
  â”œâ”€ Reads: import.meta.env.VITE_API_KEY
  â”œâ”€ Makes: fetch(API_BASE_URL + '/api/analyze', {
  â”‚         method: 'POST',
  â”‚         headers: { Authorization: `Bearer ${API_KEY}` },
  â”‚         body: JSON.stringify({ product_url: currentProductUrl })
  â”‚       })
  â”œâ”€ Stores: state.data = await response.json()
  â”œâ”€ Extracts: harmScore = state.data.analysis.product_analysis.overall_score
  â””â”€ Calls: injectTriggerButton(harmScore)

ğŸ“„ extension/src/content/content.ts::injectTriggerButton(score: number)
  â”œâ”€ Creates: <button> element with donut chart SVG
  â”œâ”€ Attaches: click event â†’ openSidebar()
  â”œâ”€ Injects: Button into page DOM (above product title)
  â””â”€ Returns: void

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 2: BACKEND API PROCESSING (Concurrent with Step 1)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[HTTP POST /api/analyze arrives at FastAPI]
  â†“
ğŸ“„ backend/src/api/main.py::app (FastAPI application)
  â”œâ”€ Middleware: CORS handler
  â”œâ”€ Routes: /api/health â†’ health.router
  â””â”€ Routes: /api/analyze â†’ analyze.router

ğŸ“„ backend/src/api/routes/analyze.py::analyze_product(
      request: AnalysisRequest,
      credentials: HTTPAuthorizationCredentials
    ) â†’ AnalysisResponse
  â”‚
  â”œâ”€ Calls: verify_api_key(credentials) â†’ None | raises HTTPException
  â”‚   â””â”€ ğŸ“„ backend/src/api/auth.py::verify_api_key(credentials)
  â”‚       â”œâ”€ Reads: settings.api_key
  â”‚       â”œâ”€ Compares: credentials.credentials == settings.api_key
  â”‚       â””â”€ Returns: None OR raises HTTPException(401)
  â”‚
  â”œâ”€ Initializes: db = DatabaseService()
  â”œâ”€ Generates: url_hash = db.generate_url_hash(request.product_url)
  â”‚   â””â”€ ğŸ“„ backend/src/infrastructure/database.py::generate_url_hash(url: str) â†’ str
  â”‚       â””â”€ Returns: hashlib.sha256(url.encode()).hexdigest()
  â”‚
  â”œâ”€ Checks Cache: cached = db.get_cached_analysis(url_hash)
  â”‚   â””â”€ ğŸ“„ backend/src/infrastructure/database.py::get_cached_analysis(url_hash: str) â†’ Dict | None
  â”‚       â”œâ”€ Calls: supabase.table('product_analyses').select('*').eq('url_hash', url_hash).execute()
  â”‚       â””â”€ Returns: data[0] if exists else None
  â”‚
  â”œâ”€ IF CACHE HIT:
  â”‚   â””â”€ Returns: AnalysisResponse(cached data)
  â”‚
  â”œâ”€ IF CACHE MISS - SCRAPING PATH:
  â”‚   â”‚
  â”‚   â”œâ”€ Initializes: scraper_service = ProductScraperService()
  â”‚   â”œâ”€ Calls: scraped = scraper_service.try_scrape(product_url)
  â”‚   â”‚   â””â”€ ğŸ“„ backend/src/infrastructure/product_scraper.py::try_scrape(url: str) â†’ ScrapedProduct | None
  â”‚   â”‚       â”œâ”€ Calls: scraper = ScraperFactory.get_scraper(url)
  â”‚   â”‚       â”‚   â””â”€ ğŸ“„ backend/src/infrastructure/scrapers/factory.py::get_scraper(url: str) â†’ BaseScraper | None
  â”‚   â”‚       â”‚       â”œâ”€ Checks: if 'amazon.com' in url
  â”‚   â”‚       â”‚       â””â”€ Returns: AmazonScraper() OR None
  â”‚   â”‚       â”‚
  â”‚   â”‚       â”œâ”€ IF scraper exists:
  â”‚   â”‚       â”‚   â””â”€ Calls: scraper.scrape(url)
  â”‚   â”‚       â”‚       â””â”€ ğŸ“„ backend/src/infrastructure/scrapers/amazon.py::scrape(url: str) â†’ ScrapedProduct
  â”‚   â”‚       â”‚           â”œâ”€ Makes: httpx.AsyncClient().get(url, headers={...})
  â”‚   â”‚       â”‚           â”œâ”€ Parses: soup = BeautifulSoup(html, 'lxml')
  â”‚   â”‚       â”‚           â”œâ”€ Extracts: title = soup.select_one('#productTitle').text
  â”‚   â”‚       â”‚           â”œâ”€ Extracts: brand = soup.select_one('#bylineInfo').text
  â”‚   â”‚       â”‚           â”œâ”€ Extracts: ingredients = find sections with keywords
  â”‚   â”‚       â”‚           â”œâ”€ Calculates: confidence score (0-1)
  â”‚   â”‚       â”‚           â””â”€ Returns: ScrapedProduct(raw_html_product, confidence)
  â”‚   â”‚       â”‚
  â”‚   â”‚       â””â”€ Returns: ScrapedProduct OR None (if failed)
  â”‚   â”‚
  â”‚   â”œâ”€ IF scraped AND confidence > 0.3:
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ Calls: product_data = claude_query.extract_product_data(scraped.raw_html_product)
  â”‚   â”‚   â”‚   â””â”€ ğŸ“„ backend/src/infrastructure/claude_query.py::extract_product_data(html: str) â†’ Dict
  â”‚   â”‚   â”‚       â”œâ”€ Initializes: client = Anthropic(api_key=settings.anthropic_api_key)
  â”‚   â”‚   â”‚       â”œâ”€ Calls: response = client.messages.create(
  â”‚   â”‚   â”‚       â”‚         model="claude-sonnet-4-5-20250929",
  â”‚   â”‚   â”‚       â”‚         messages=[{role: "user", content: prompt + html}],
  â”‚   â”‚   â”‚       â”‚         max_tokens=4096
  â”‚   â”‚   â”‚       â”‚       )
  â”‚   â”‚   â”‚       â”œâ”€ Parses: JSON from response.content[0].text
  â”‚   â”‚   â”‚       â””â”€ Returns: {product_name, brand, ingredients, materials, ...}
  â”‚   â”‚   â”‚
  â”‚   â”‚   â””â”€ Calls: analysis_data = claude_agent.analyze_extracted_product(product_data, url)
  â”‚   â”‚       â””â”€ ğŸ“„ backend/src/infrastructure/claude_agent.py::analyze_extracted_product(
  â”‚   â”‚                 product_data: Dict, url: str
  â”‚   â”‚             ) â†’ Dict
  â”‚   â”‚           â”œâ”€ Initializes: client = Anthropic(api_key=settings.anthropic_api_key)
  â”‚   â”‚           â”œâ”€ Defines: tools = [web_search_tool]
  â”‚   â”‚           â”œâ”€ Calls: response = client.messages.create(
  â”‚   â”‚           â”‚         model="claude-sonnet-4-5-20250929",
  â”‚   â”‚           â”‚         messages=[{role: "user", content: prompt + product_data}],
  â”‚   â”‚           â”‚         tools=[web_search],
  â”‚   â”‚           â”‚         max_tokens=8192
  â”‚   â”‚           â”‚       )
  â”‚   â”‚           â”œâ”€ Handles: Tool use loop (web_search requests)
  â”‚   â”‚           â”‚   â””â”€ For each web_search:
  â”‚   â”‚           â”‚       â”œâ”€ Makes: httpx.get('https://google.serper.dev/search', ...)
  â”‚   â”‚           â”‚       â””â”€ Returns: search results to Claude
  â”‚   â”‚           â”œâ”€ Parses: Final JSON response
  â”‚   â”‚           â””â”€ Returns: {allergens_detected, pfas_detected, other_concerns, confidence}
  â”‚   â”‚
  â”‚   â”œâ”€ IF scraping FAILED OR low confidence:
  â”‚   â”‚   â””â”€ Calls: analysis_data = claude_agent.analyze_product(product_url)
  â”‚   â”‚       â””â”€ ğŸ“„ backend/src/infrastructure/claude_agent.py::analyze_product(url: str) â†’ Dict
  â”‚   â”‚           â”œâ”€ Similar to analyze_extracted_product but with web_fetch tool
  â”‚   â”‚           â”œâ”€ Claude fetches the product page itself
  â”‚   â”‚           â””â”€ Returns: analysis_data
  â”‚   â”‚
  â”‚   â”œâ”€ Calculates: harm_score = HarmScoreCalculator.calculate(analysis_data)
  â”‚   â”‚   â””â”€ ğŸ“„ backend/src/domain/harm_calculator.py::HarmScoreCalculator.calculate(
  â”‚   â”‚             analysis_data: Dict[str, Any]
  â”‚   â”‚         ) â†’ int
  â”‚   â”‚       â”œâ”€ Initializes: total_score = 0
  â”‚   â”‚       â”œâ”€ For allergens_detected:
  â”‚   â”‚       â”‚   â””â”€ Adds: severity points (5-30 per allergen)
  â”‚   â”‚       â”œâ”€ For pfas_detected:
  â”‚   â”‚       â”‚   â””â”€ Adds: 40 points per PFAS compound
  â”‚   â”‚       â”œâ”€ For other_concerns:
  â”‚   â”‚       â”‚   â””â”€ Adds: points based on toxicity level
  â”‚   â”‚       â”œâ”€ Applies: category multipliers (pesticides, cleaners)
  â”‚   â”‚       â”œâ”€ Caps: max(min(total_score, 100), 0)
  â”‚   â”‚       â””â”€ Returns: harm_score (0-100)
  â”‚   â”‚
  â”‚   â”œâ”€ Builds: analysis = ProductAnalysis(
  â”‚   â”‚           product_name=...,
  â”‚   â”‚           overall_score=100 - harm_score,
  â”‚   â”‚           allergens_detected=...,
  â”‚   â”‚           pfas_detected=...,
  â”‚   â”‚           ...
  â”‚   â”‚         )
  â”‚   â”‚
  â”‚   â”œâ”€ Stores: db.store_analysis(url_hash, product_url, analysis_response)
  â”‚   â”‚   â””â”€ ğŸ“„ backend/src/infrastructure/database.py::store_analysis(
  â”‚   â”‚             url_hash: str, url: str, response: AnalysisResponse
  â”‚   â”‚         ) â†’ bool
  â”‚   â”‚       â”œâ”€ Prepares: db_data = {url_hash, product_url, product_name, ...}
  â”‚   â”‚       â”œâ”€ Calls: supabase.table('product_analyses').upsert(db_data).execute()
  â”‚   â”‚       â””â”€ Returns: True OR False (on error)
  â”‚   â”‚
  â”‚   â””â”€ Returns: AnalysisResponse(
  â”‚               analysis=analysis,
  â”‚               alternatives=[],
  â”‚               cached=False,
  â”‚               url_hash=url_hash
  â”‚             )

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 3: USER CLICKS TRIGGER BUTTON â†’ SIDEBAR OPENS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[User clicks floating donut chart button]
  â†“
ğŸ“„ extension/src/content/content.ts::openSidebar()
  â”œâ”€ Creates: iframe = document.createElement('iframe')
  â”œâ”€ Sets: iframe.src = chrome.runtime.getURL('src/sidebar.html')
  â”œâ”€ Injects: document.body.appendChild(iframe)
  â”œâ”€ Waits: iframe onload event
  â”œâ”€ Sends: iframe.contentWindow.postMessage({
  â”‚          type: 'ANALYSIS_DATA',
  â”‚          data: state.data
  â”‚        }, '*')
  â””â”€ Hides: trigger button (display: none)

ğŸ“„ extension/src/sidebar.html (loaded in iframe)
  â””â”€ Loads: <script type="module" src="/sidebar.js"></script>

ğŸ“„ extension/src/sidebar.ts::initApp()
  â”œâ”€ Gets: app = document.getElementById('app')
  â””â”€ Calls: mount(Sidebar, { target: app })

ğŸ“„ extension/src/Sidebar.svelte::onMount()
  â”œâ”€ Sets up: chrome.runtime.onMessage listener
  â”œâ”€ Sets up: window.addEventListener('message', handleMessage)
  â””â”€ Defines: handleMessage(event: MessageEvent)

ğŸ“„ extension/src/Sidebar.svelte::handleMessage(event)
  â”œâ”€ Checks: if (event.data.type === 'ANALYSIS_DATA')
  â”œâ”€ Extracts: data = event.data.data
  â”œâ”€ Sets: analysis = data (reactive state)
  â”œâ”€ Sets: loading = false
  â”œâ”€ Calls: cache.set(productUrl, data)
  â”‚   â””â”€ ğŸ“„ extension/src/lib/cache.ts::set(key: string, value: AnalysisResponse)
  â”‚       â”œâ”€ Opens: db = await openDB('eject-cache', 1)
  â”‚       â”œâ”€ Stores: db.put('analyses', { key, value, timestamp })
  â”‚       â””â”€ Returns: void
  â””â”€ Renders: <Sidebar {analysis} />

ğŸ“„ extension/src/components/Sidebar.svelte (UI Component)
  â”œâ”€ Receives: analysis prop (AnalysisResponse)
  â”œâ”€ Extracts: productAnalysis = analysis.analysis.product_analysis
  â”œâ”€ Computes: harmScore = getHarmScore(productAnalysis)
  â”‚   â””â”€ ğŸ“„ extension/src/lib/utils.ts::getHarmScore(analysis: ProductAnalysis) â†’ number
  â”‚       â””â”€ Returns: 100 - analysis.overall_score
  â”‚
  â”œâ”€ Computes: riskLevel = getRiskLevel(harmScore)
  â”‚   â””â”€ ğŸ“„ extension/src/lib/utils.ts::getRiskLevel(score: number) â†’ RiskLevel
  â”‚       â”œâ”€ Returns: 'low' if score < 30
  â”‚       â”œâ”€ Returns: 'medium' if score < 60
  â”‚       â””â”€ Returns: 'high' if score >= 60
  â”‚
  â”œâ”€ Computes: riskClass = getRiskClass(riskLevel)
  â”‚   â””â”€ ğŸ“„ extension/src/lib/utils.ts::getRiskClass(level: RiskLevel) â†’ string
  â”‚       â””â”€ Returns: CSS class name ('risk-low' | 'risk-medium' | 'risk-high')
  â”‚
  â”œâ”€ Renders: Donut chart SVG with harmScore
  â”œâ”€ Renders: Product name and brand
  â”œâ”€ Renders: Allergens list (if any)
  â”œâ”€ Renders: PFAS list (if any)
  â”œâ”€ Renders: Other concerns list (if any)
  â””â”€ Renders: Confidence score and timestamp
```

---

## File-Level Import Graph

### Backend Dependencies

```
run.py
  â””â”€ src.infrastructure.config.settings

src/api/main.py
  â”œâ”€ src.infrastructure.config.settings
  â””â”€ src.api.routes.{health, analyze}

src/api/auth.py
  â””â”€ src.infrastructure.config.settings

src/api/routes/health.py
  â””â”€ (no internal imports)

src/api/routes/analyze.py
  â”œâ”€ src.domain.models.*
  â”œâ”€ src.domain.harm_calculator.HarmScoreCalculator
  â”œâ”€ src.infrastructure.claude_agent.ProductSafetyAgent
  â”œâ”€ src.infrastructure.product_scraper.ProductScraperService
  â”œâ”€ src.infrastructure.claude_query.ClaudeQueryService
  â”œâ”€ src.infrastructure.database.DatabaseService
  â””â”€ src.api.auth.verify_api_key

src/domain/models.py
  â””â”€ (no internal imports - pure Pydantic models)

src/domain/harm_calculator.py
  â””â”€ (no internal imports - pure logic)

src/infrastructure/config.py
  â””â”€ (no internal imports - Pydantic settings)

src/infrastructure/database.py
  â””â”€ src.infrastructure.config.settings

src/infrastructure/claude_agent.py
  â””â”€ src.infrastructure.config.settings

src/infrastructure/claude_query.py
  â”œâ”€ src.infrastructure.config.settings
  â””â”€ src.domain.models.ScrapedProduct

src/infrastructure/product_scraper.py
  â”œâ”€ src.infrastructure.scrapers.factory.ScraperFactory
  â””â”€ src.domain.models.ScrapedProduct

src/infrastructure/scrapers/factory.py
  â”œâ”€ src.infrastructure.scrapers.base.BaseScraper
  â””â”€ src.infrastructure.scrapers.amazon.AmazonScraper

src/infrastructure/scrapers/base.py
  â””â”€ (abstract base - no imports)

src/infrastructure/scrapers/amazon.py
  â”œâ”€ src.infrastructure.scrapers.base.BaseScraper
  â””â”€ src.domain.models.ScrapedProduct
```

### Extension Dependencies

```
sidebar.ts
  â”œâ”€ ./app.css
  â””â”€ ./Sidebar.svelte

Sidebar.svelte
  â”œâ”€ ./components/Sidebar.svelte
  â”œâ”€ ./lib/api.{api}
  â”œâ”€ ./lib/cache.{cache}
  â””â”€ ./types.{AnalysisResponse}

components/Sidebar.svelte
  â”œâ”€ @/types.*
  â””â”€ @/lib/utils.*

content/content.ts
  â””â”€ (no file imports - uses chrome API and import.meta.env)

background/background.ts
  â””â”€ (no file imports - uses chrome API)

lib/api.ts
  â””â”€ @/types.{AnalysisResponse}

lib/cache.ts
  â””â”€ @/types.{AnalysisResponse, CachedAnalysis}

lib/utils.ts
  â””â”€ @/types.{ProductAnalysis, RiskLevel}

types/index.ts
  â””â”€ (no imports - pure type definitions)
```

---

## Cross-Directory Relationships

### Backend: Clean Architecture Pattern

```
API Layer (src/api/)
  â†“ calls
Domain Layer (src/domain/)
  â†“ uses
Infrastructure Layer (src/infrastructure/)
  â†“ calls
External Services (Anthropic, Supabase, Web)
```

**Key Dependency Rule**: Higher layers depend on lower layers. Infrastructure is called by Domain/API but never calls them back (dependency inversion).

### Extension: Component-Based Architecture

```
Content Script (content/)
  â†“ creates
Sidebar App (Sidebar.svelte)
  â†“ uses
Libraries (lib/)
  â†“ uses
Types (types/)
```

**Key Pattern**: Content script is isolated (no imports) to avoid bundling issues. Sidebar app handles all state management and API communication.

---

## Bloat Identification

### âš ï¸ BLOAT: Legacy Migrations

**Location**: `/backend/migrations/`

**Evidence**:
- Contains outdated SQL files: `001_create_tables.sql`, `002_seed_knowledge_base.sql`
- Superseded by `/backend/supabase/migrations/`
- Old schema missing tables (toxic_substances) and columns

**Impact**: None (not used in production)

### âš ï¸ BLOAT: Unused Application Layer

**Location**: `/backend/src/application/`

**Evidence**:
- Directory contains only empty `__init__.py`
- No code implements application layer pattern
- Business logic exists in `domain/` and `infrastructure/`

**Impact**: None (empty directory)

### âš ï¸ DEVELOPMENT SCAFFOLDING: Empty Test Directories

**Locations**:
- `/backend/tests/unit/` (empty except `__init__.py`)
- `/backend/tests/integration/` (empty except `__init__.py`)

**Evidence**:
- Only E2E tests implemented (`tests/e2e/test_product_analysis.py`)
- Unit and integration test directories created but unused

**Impact**: None (future test scaffolding)

---

## Subdirectory Documentation

### Backend
- [Backend Overview](./backend/CLAUDE.md) - FastAPI server, clean architecture, Claude AI integration

### Extension
- [Extension Overview](./extension/CLAUDE.md) - Svelte 5 Chrome extension, Manifest V3, UI components

---

## Essential Files Summary

**Total Source Files**: 52
- **Backend**: 21 Python files
- **Extension**: 12 TypeScript/Svelte files
- **Database**: 4 SQL migration files (active)
- **Tests**: 4 test files
- **Bloat**: 3 files (5.8%)

**Codebase Health**: 94.2% of files are actively used and essential for the product to function.

---

## Key Technologies

### Backend Stack
- **Framework**: FastAPI (Python)
- **AI**: Anthropic Claude Sonnet 4.5 with Agent SDK
- **Database**: Supabase (PostgreSQL)
- **Scraping**: httpx + BeautifulSoup4
- **Testing**: pytest

### Extension Stack
- **Framework**: Svelte 5
- **Language**: TypeScript
- **Build**: Vite
- **Styling**: Tailwind CSS
- **Storage**: IndexedDB (idb library)
- **Manifest**: Chrome Extension Manifest V3

---

Last Updated: 2025-11-18
